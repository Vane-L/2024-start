source link: [限流的底层原理解析](https://mp.weixin.qq.com/s/EJ68f40ebapdqKTgGOG8tw)

# 限流主要有两个目的
1. 防止系统过载：确保系统在高负载情况下仍能保持稳定运行。
2. 保证服务质量：为所有用户提供公平的服务，避免某些用户占用过多资源。

# 限流算法的实现
## 固定窗口计数器算法
实现原理：固定窗口计数器算法通过设置一个固定的时间窗口（例如每分钟）和一个在这个窗口内允许的请求数量限制（例如10个请求）。在每个时间窗口开始时，计数器重置为零，随着请求的到来，计数器递增。当计数器达到限制时，后续的请求将被拒绝，直到窗口重置。

固定窗口计数器算法适用于**请求分布相对均匀**的场景，但在请求可能在短时间内集中到达的场景下，可能需要考虑更复杂的限流算法，如滑动窗口或令牌桶算法。

优点：
- 实现简单直观。
- 容易理解和实现。
- 可以保证在任何给定的固定时间窗口内，请求的数量不会超过设定的阈值。

缺点：
- 在窗口切换的瞬间可能会有请求高峰，因为计数器重置可能导致大量请求几乎同时被处理。
- 无法平滑地处理突发流量，可能导致服务体验不佳。

```go
// FixedWindowCounter 结构体实现固定窗口计数器限流算法。
// mu 用于同步访问，保证并发安全。
// count 记录当前时间窗口内的请求数量。
// limit 是时间窗口内允许的最大请求数量。
// window 记录当前时间窗口的开始时间。
// duration 是时间窗口的持续时间。
type FixedWindowCounter struct {
     mu        sync.Mutex
     count     int
     limit     int
     window    time.Time
     duration  time.Duration
}

// Allow 方法用于判断当前请求是否被允许。
// 首先通过互斥锁保证方法的原子性。
func (f *FixedWindowCounter) Allow() bool {
    f.mu.Lock()
    defer f.mu.Unlock()
    
    now := time.Now() // 获取当前时间。
    
    // 如果当前时间超过了窗口的结束时间，重置计数器和窗口开始时间。
    if now.After(f.window.Add(f.duration)) {
    f.count = 0
    f.window = now
    }
    
    // 如果当前计数小于限制，则增加计数并允许请求。
    if f.count < f.limit {
    f.count++
    return true
    }
    // 如果计数达到限制，则拒绝请求。
    return false
}
```

## 滑动窗口计数器算法
实现原理：滑动窗口算法通过将时间分为多个小的时间段，每个时间段内维护一个独立的计数器。当一个请求到达时，它会被分配到当前时间所在的小时间段，并检查该时间段的计数器是否已达到限制。如果未达到，则允许请求并增加计数；如果已达到，则拒绝请求。随着时间的推移，旧的时间段会淡出窗口，新的时间段会加入。

滑动窗口算法适用于**需要平滑流量控制**的场景，尤其是在面对突发流量时，能够提供比固定窗口计数器更优的流量控制效果。

优点：
- 相比固定窗口算法，滑动窗口算法能够更平滑地处理请求，**避免瞬时高峰**。
- 可以提供更细致的流量控制。

缺点：
- 实现相对复杂，需要维护多个计数器和时间索引。
- 对内存和计算的要求更高。


```go
// SlidingWindowLimiter 结构体实现滑动窗口限流算法。
type SlidingWindowLimiter struct {
     mutex       sync.Mutex
     counters    []int
     limit       int
     windowStart time.Time
     windowDuration time.Duration
     interval    time.Duration
}
// Allow 方法用于判断当前请求是否被允许，并实现滑动窗口的逻辑。
func (s *SlidingWindowLimiter) Allow() bool {
    s.mutex.Lock()
    defer s.mutex.Unlock()
    
    // 检查是否需要滑动窗口
    if time.Since(s.windowStart) > s.windowDuration {
        s.slideWindow()
    }
    
    now := time.Now()
    index := int((now.UnixNano() - s.windowStart.UnixNano()) / s.interval.Nanoseconds()) % len(s.counters)
    if s.counters[index] < s.limit {
        s.counters[index]++
        return true
    }
    return false
}
// slideWindow 方法实现滑动窗口逻辑，移除最旧的时间段并重置计数器。
func (s *SlidingWindowLimiter) slideWindow() {
    // 滑动窗口，忽略最旧的时间段
    copy(s.counters, s.counters[1:])
    // 重置最后一个时间段的计数器
    s.counters[len(s.counters)-1] = 0
    // 更新窗口开始时间
    s.windowStart = time.Now()
}
```

## 漏桶算法
实现原理：通过一个固定容量的队列来模拟桶，以**恒定速率**从桶中取出请求进行处理，无论请求到达的频率如何，都保证请求以均匀的速度被处理，从而平滑流量并防止流量突增。

漏桶算法适用于**需要强制执行固定速率处理**的场景，如网络流量控制、API请求限制等。通过控制令牌的添加速率，漏桶算法能够有效地避免系统因瞬时流量高峰而过载。

优点：
- 能够强制实现固定的数据处理速率，平滑流量。
- 即使面对突发流量，也能保持稳定的处理速率。

缺点：
- 对于突发流量的处理不够灵活，可能会延迟处理。
- 实现相对简单，但需要维护桶的状态。

```go
// LeakyBucket 结构体，包含请求队列
type LeakyBucket struct {
    queue chan struct{} // 请求队列
}
// NewLeakyBucket 创建一个新的漏桶实例
func NewLeakyBucket(capacity int) *LeakyBucket {
    return &LeakyBucket{
        queue: make(chan struct{}, capacity),
    }
}
// push 将请求放入队列，如果队列满了，返回 false，表示请求被丢弃
func (lb *LeakyBucket) push() bool {
    // 如果通道可以发送，请求被接受
    select {
        case lb.queue <- struct{}{}:
            return true
        default:
            return false
    }
}
// process 从队列中取出请求并模拟处理过程
func (lb *LeakyBucket) process() {
    for range lb.queue { // 使用 range 来持续接收队列中的请求
        fmt.Println("Request processed at", time.Now().Format("2006-01-02 15:04:05"))
        time.Sleep(100 * time.Millisecond) // 模拟请求处理时间
    }
}
```

## 令牌桶算法
实现原理：令牌桶算法使用一个令牌桶来调节数据流的速率，**允许一定程度的流量突发**。桶初始时为空，并以固定的速率填充令牌，直至达到预设的容量上限。
- 与漏桶算法不同，令牌桶算法在桶未满时，可以在每个时间间隔内向桶中添加多个令牌，从而积累处理突发请求的能力。当请求到达时，如果桶中存在令牌，算法会从桶中移除相应数量的令牌来处理请求。如果桶中的令牌不足，请求将被延迟处理或根据策略拒绝服务。如果桶已满，额外的令牌将不会被添加，确保了令牌数量不会超过桶的容量限制。

令牌桶算法适用于**需要处理突发流量**的场景，如网络通信、API调用等。通过控制令牌的填充速率和桶的容量，令牌桶算法能够有效地平衡流量，防止系统过载，同时允许在短期内处理更多的请求。

优点：
- 允许一定程度的突发流量，更加灵活。
- 可以平滑流量，同时在桶未满时快速处理请求。

缺点：
- 实现相对复杂，需要维护桶的状态和时间。
- 对于计算和同步的要求更高。


```go
// TokenBucket 结构体实现令牌桶限流算法。
// - mu 用于同步访问，保证并发安全。
// - capacity 定义桶的容量，即桶中最多可以存放的令牌数。
// - tokens 表示桶中当前的令牌数。
// - refillRate 是令牌的填充速率，表示每秒向桶中添加的令牌数。
// - lastRefill 记录上次填充令牌的时间。
type TokenBucket struct {
    mu         sync.Mutex
    capacity   int
    tokens     int
    refillRate float64
    lastRefill time.Time
}
// Allow 方法用于判断当前请求是否被允许。
func (t *TokenBucket) Allow() bool {
    t.mu.Lock() // 进入临界区，确保操作的原子性。
    defer t.mu.Unlock()
    
    now := time.Now() // 获取当前时间。
    
    // 计算自上次填充以来经过的秒数，并转换为float64类型。
    timeElapsed := float64(now.Unix() - t.lastRefill.Unix())
    
    // 根据 refillRate 计算应该添加的令牌数。
    tokensToAdd := t.refillRate * timeElapsed
    
    // 更新令牌数，但不超过桶的容量。
    t.tokens += int(tokensToAdd)
    if t.tokens > t.capacity {
        t.tokens = t.capacity // 确保令牌数不超过桶的容量。
    }
    
    // 如果桶中有令牌，则移除一个令牌并允许请求通过。
    if t.tokens > 0 {
        t.tokens--         // 移除一个令牌。
        t.lastRefill = now // 更新上次填充时间到当前时间。
        return true
    }
    
    // 如果桶中无令牌，则请求被拒绝。
    return false
}

```

# 限流的实现方式
## 应用层限流
应用层限流是在应用程序的代码中直接实现限流逻辑，这通常是通过使用中间件来完成的。中间件可以在处理请求之前先进行限流检查，以决定是否继续处理请求或者返回错误信息。

应用层限流适用于需要细粒度控制的场景，允许开发者根据具体的业务需求定制限流策略。通过合理配置限流器的参数，可以在保证服务质量的同时，提高应用程序的吞吐量和稳定性。

优点：
- 易于实现和集成，可以轻松地添加到现有的Web应用程序中。
- 细粒度控制，可以针对不同的路由或用户应用不同的限流策略。

缺点：
- 可能会增加请求处理的延迟，因为中间件需要在每次请求时进行同步操作。
- 如果不恰当地使用，可能会降低应用程序的并发处理能力。

## 代理层限流
实现原理：在Nginx中，通过定义limit_req_zone指令创建一个限流区域，并指定使用共享内存来存储客户端IP地址和对应的请求计数。rate参数定义了每个客户端每秒钟允许的请求数量。在server块中，使用limit_req指令引用之前定义的限流区域，并设置burst参数允许一定数量的突发请求。

代理层限流适用于需要在多个服务或整个应用层面控制请求的场景。通过合理配置代理服务器的限流规则，可以在不同的层面上保护系统，提高整体的稳定性和可用性。

优点：
- 在网络层面进行限流，可以保护所有后端服务，而不需要在每个应用程序中单独实现限流逻辑。
- 减轻了后端服务的负担，因为多余的请求在到达后端之前就被拒绝了。
- 配置灵活，可以针对不同的请求路径和客户端设置不同的限流规则。

缺点：
- 需要代理服务器支持限流功能，可能需要额外的配置和调优。
- 对于分布式系统，可能需要额外的机制来同步状态，确保全局的限流效果。

```editorconfig
http {
    # 定义一个限流区域，使用共享内存存储状态
    limit_req_zone $binary_remote_addr zone=mylimit:10m rate=1r/s;

    server {
        # 监听80端口
        listen 80;

        # 定义一个location块，用于匹配特定的请求路径
        location /api/ {
            # 应用限流规则
            limit_req zone=mylimit burst=5 nodelay;

            # 代理请求到后端服务
            proxy_pass http://backend/;
        }
    }
}
```